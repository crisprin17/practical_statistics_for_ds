{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% confidence interval = we can expect that 95 out of 100 our estimate include the tru population value\n",
    "\n",
    "= mean +- ME = mean +- critical_score * SE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence interval by hand for sample mean\n",
    "There are two common ways that interviewers will touch on confidence intervals; they will either ask you to explain it in simple terms, or elaborate on how they are calculated, possibly having you implement one. In this exercise, you'll practice the latter by producing a confidence interval by hand, using no packages other than those imported for you.\n",
    "\n",
    "We have gone ahead and assigned the appropriate z-score for a 95% confidence interval and sample mean to the z_score and sample_mean variables to simplify things a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem, t\n",
    "data = [1, 2, 3, 4, 5]\n",
    "confidence = 0.95\n",
    "\n",
    "# Compute the standard error and margin of error\n",
    "std_err = sem(data)\n",
    "margin_error = std_err * z_score\n",
    "\n",
    "# Compute and print the lower threshold\n",
    "lower = sample_mean - margin_error\n",
    "print(lower)\n",
    "\n",
    "# Compute and print the upper threshold\n",
    "upper = sample_mean + margin_error\n",
    "print(upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying confidence intervals for proportion\n",
    "In practice, you aren't going to hand-code confidence intervals. Let's utilize the statsmodels package to streamline this process and examine some more tendencies of interval estimates.\n",
    "\n",
    "In this exercise, we've generated a binomial sample of the number of heads in 50 fair coin flips saved as the heads variable. You'll compute a few different confidence intervals for this sample, and then scale your work for 10 similar samples.\n",
    "\n",
    "The proportion_confint() function has already been imported to help you compute confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.21148951611838032, 0.4285104838816197)\n",
      "(0.1934013015689181, 0.4065986984310819)\n",
      "(0.46518968814451866, 0.6948103118554813)\n",
      "(0.26709065248750397, 0.49290934751249604)\n",
      "(0.507090652487504, 0.732909347512496)\n",
      "(0.3836912846323326, 0.6163087153676674)\n",
      "(0.42406406993539053, 0.6559359300646095)\n",
      "(0.48604119788424416, 0.7139588021157558)\n",
      "(0.30518968814451874, 0.5348103118554812)\n",
      "(0.42406406993539053, 0.6559359300646095)\n"
     ]
    }
   ],
   "source": [
    "#Compute and print a 99% confidence interval for 50 trials; does it contain the true proportion of a fair coin flip?\n",
    "# Compute and print the 99% confidence interval\n",
    "# Repeat this process 10 times \n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from scipy.stats import binom\n",
    "heads = binom.rvs(50, 0.5, size=10)\n",
    "for val in heads:\n",
    "    confidence_interval = proportion_confint(val, 50, .10)\n",
    "    print(confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "## Assumption\n",
    "1) random sample\n",
    "\n",
    "2) population normally. distributed\n",
    "\n",
    "3) observation need to be indipendent\n",
    "\n",
    "4) variance needs to be constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One tailed z-test: proportion of conversion\n",
    "\n",
    "We know now that hypothesis tests can come in several forms. In this exercise, you'll implement a one tailed z-test on test data from tracking conversion on a mobile app. The data has been imported as results and numpy has already been imported for you along with pandas as well.\n",
    "\n",
    "The treatment group represents some graphic alteration that we expect to improve the conversion rate of users. Run a test with alpha as .05 and find out if the change actually helped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign and print the conversion rate for each group\n",
    "conv_rates = results.groupby(results.Group).mean()\n",
    "print(conv_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the number of control conversions to num_control and the total number of trials to the total_control variable by slicing the DataFrame.\n",
    "num_control = results[results['Group']=='control']['Converted'].sum()\n",
    "total_control = len(results[results['Group']=='control'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the number of conversions and total trials\n",
    "num_treat = results[results['Group']=='treatment']['Converted'].sum()\n",
    "total_treat = len(results[results['Group']=='treatment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the z-test using the proportions_ztest() function and passing it the count and nobs variables.\n",
    "\n",
    "alternative : string in [‘two-sided’, ‘smaller’, ‘larger’]\n",
    "\n",
    "The alternative hypothesis can be either two-sided or one of the one- sided tests, smaller means that the alternative hypothesis is prop < value` and larger means ``prop > value, or the corresponding inequality for the two sample test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "count = np.array([num_treat, num_control]) \n",
    "nobs = np.array([total_treat, total_control])\n",
    "\n",
    "# Run the z-test and print the result \n",
    "stat, pval = proportions_ztest(count, nobs, alternative=\"larger\")\n",
    "print('{0:0.3f}'.format(pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two tailed t-test\n",
    "In this exercise, you'll tackle another type of hypothesis test with the two tailed t-test for means. More concretely, you'll run the test on our laptops dataset from before and try to identify a significant difference in price between Asus and Toshiba.\n",
    "\n",
    "Once again, we've imported all of the standard packages. Once you get your result, don't forget to make an actionable conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign and print the mean price for each group using the groupby() function on the Company feature.\n",
    "prices = laptops.groupby(laptops['Company']).mean()\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the prices of each group\n",
    "asus = laptops[laptops['Company'] == 'Asus']['Price']\n",
    "toshiba = laptops[laptops['Company'] == 'Toshiba']['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the t-test\n",
    "from scipy.stats import ttest_ind\n",
    "tstat, pval = ttest_ind(asus, toshiba)\n",
    "print('{0:0.3f}'.format(pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a p-value of .133, we cannot reject the null hypothesis! There's not enough evidence here to conclude that Toshiba laptops are significantly more expensive than Asus. With that being said, .133 is fairly close to reasonable significance so we may want to run another test or examine this further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power and sample size\n",
    "1) Power: probability of detectiong the effect, higher power less Type II error\n",
    "2) confidence level\n",
    "3) minimum effect size\n",
    "\n",
    "***Sample size and confidence level are negatively correlated with Type II error, while minimum effect size causes a higher chance of Type II error.***\n",
    "\n",
    "\n",
    "-> standardize effect size required for those package: example increase from 0.2 -  0.25 = 0.5\n",
    "REMEMBER:\n",
    "1. The smaller the effect that you want to detect, the higher the probability of a Type II error.\n",
    "2. The larger sample size, the less probable a Type II error is.\n",
    "3. The higher the confidence level, the lower the chances of a Type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating sample size\n",
    "Let's finish up our dive into statistical tests by performing power analysis to generate needed sample size. Power analysis involves four moving parts:\n",
    "\n",
    "1. Sample size\n",
    "\n",
    "2. Effect size\n",
    "\n",
    "3. Minimum effect\n",
    "\n",
    "4. Power\n",
    "\n",
    "In this exercise, you're working with a website and want to test for a difference in conversion rate. Before you begin the experiment, you must decide how many samples you'll need per variant using 5% significance and 95% power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the effect of a conversion rate increase from 20% to 25% success using the proportion_effectsize() function.\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "std_effect = proportion_effectsize(0.2, 0.25)\n",
    "\n",
    "# Assign and print the needed sample size\n",
    "from statsmodels.stats.power import  zt_ind_solve_power\n",
    "sample_size = zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=0.05, power=0.95)\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonferroni correction\n",
    "Bonferroni correction\n",
    "Let's implement multiple hypothesis tests using the Bonferroni correction approach that we discussed in the slides. You'll use the imported multipletests() function in order to achieve this.\n",
    "\n",
    "Use a single-test significance level of .05 and observe how the Bonferroni correction affects our sample list of p-values already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "pvals = [.01, .05, .10, .50, .99]\n",
    "\n",
    "# Create a list of the adjusted p-values\n",
    "p_adjusted = multipletests(pvals, alpha=0.05, method='bonferroni')\n",
    "\n",
    "# Print the resulting conclusions\n",
    "print(p_adjusted[0])\n",
    "\n",
    "# Print the adjusted p-values themselves \n",
    "print(p_adjusted[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
